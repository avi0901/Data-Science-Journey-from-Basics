{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c2bd7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dcfe24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#AND function inputs\n",
    "x = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([[0,0,0,1]]).T\n",
    "x.shape , y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df0624a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78b0e4",
   "metadata": {},
   "source": [
    "### Forward Propagation without hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e5414f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.73171152],\n",
       "        [ 0.91745384]]),\n",
       " array([-0.29585281]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2*np.random.random((2,1)) - 1\n",
    "bias = 2*np.random.random(1) - 1\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f667d7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.42657161],\n",
       "       [0.65058259],\n",
       "       [0.26355658],\n",
       "       [0.47250016]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output0 = x\n",
    "output = sig(np.dot(output0,weights)+ bias)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4b6988",
   "metadata": {},
   "source": [
    " ### Forward Propagation with hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d13bea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "wh = 2 * np.random.random((2,2)) - 1    #weight hidden\n",
    "bh = 2 * np.random.random((1,2)) - 1    #bias hidden\n",
    "wo = 2 * np.random.random((2,1)) - 1    #weight output\n",
    "bo = 2 * np.random.random((1,1)) - 1    #bias output\n",
    "#np.random.random((No. of Input, No. of Fuction in layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "459f706d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.6195206 ],\n",
       "       [0.5978521 ],\n",
       "       [0.64163213],\n",
       "       [0.61904948]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output0 = x\n",
    "outputHidden = sig(np.dot(output0,wh)+ bh)\n",
    "output = sig(np.dot(outputHidden , wo) + bo)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f472bb1a",
   "metadata": {},
   "source": [
    "What we done above is just taken random values of weights and done forward propogation. We were just understanding how to do forward propogation, we haven't considered values of y yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39dbed0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1-sig(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7629c65",
   "metadata": {},
   "source": [
    "## Implementing a simple Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df4afe14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[-0.25376631],\n",
       "        [ 0.94719186]]),\n",
       " array([-0.68548258]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2*np.random.random((2,1)) - 1\n",
    "bias = 2*np.random.random(1) - 1\n",
    "lr = 0.1 #learning rate\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6815add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.46816198],\n",
       "        [5.46816197]]),\n",
       " array([-8.29498527]),\n",
       " array([[2.49703834e-04],\n",
       "        [5.58917893e-02],\n",
       "        [5.58917897e-02],\n",
       "        [9.33475144e-01]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    output0 = x\n",
    "    output = sig(np.dot(output0,weights)+ bias)\n",
    "\n",
    "    first_term = output - y\n",
    "    input_for_last_layer = np.dot(output0,weights)+ bias\n",
    "    second_term = derivativeSig(input_for_last_layer)  # it is same as output*(1-output)\n",
    "    first_two = first_term*second_term\n",
    "    first_two.shape\n",
    "\n",
    "    changes = np.array([[0.0],[0.0]])\n",
    "    # storing derivatives wrt weights\n",
    "\n",
    "    for i in range(2):\n",
    "        for j in range(4):\n",
    "            changes[i][0] += first_two[j][0] * output0[j][i]\n",
    "    weights = weights - lr * changes\n",
    "\n",
    "    bias_change = 0.0\n",
    "    for j in range(4):\n",
    "        bias_change += first_two[j][0] * 1\n",
    "    bias = bias - lr * bias_change\n",
    "    \n",
    "output = sig(np.dot(x,weights)+bias)\n",
    "weights, bias, output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cc0ba0",
   "metadata": {},
   "source": [
    "### Optimizing code using Vectors Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fa6b06bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.28678216],\n",
       "        [-0.72417446]]),\n",
       " array([0.52777548]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = 2*np.random.random((2,1)) - 1\n",
    "bias = 2*np.random.random(1) - 1\n",
    "lr = 0.1 #learning rate\n",
    "weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2629088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[5.47183128],\n",
       "        [5.47183127]]),\n",
       " array([-8.30047284]),\n",
       " array([[2.48337659e-04],\n",
       "        [5.57959205e-02],\n",
       "        [5.57959212e-02],\n",
       "        [9.33589999e-01]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for iter in range(10000):\n",
    "    output0 = x\n",
    "    output = sig(np.dot(output0,weights)+ bias)\n",
    "\n",
    "    first_term = output - y\n",
    "    input_for_last_layer = np.dot(output0,weights)+ bias\n",
    "    second_term = derivativeSig(input_for_last_layer)  # it is same as output*(1-output)\n",
    "    first_two = first_term*second_term\n",
    "    first_two.shape\n",
    "\n",
    "    changes = np.dot(output0.T, first_two)\n",
    "    \n",
    "    weights = weights - lr * changes\n",
    "\n",
    "    bias_change = np.sum(first_two)\n",
    "    bias = bias - lr * bias_change\n",
    "    \n",
    "output = sig(np.dot(x,weights)+bias)\n",
    "weights, bias, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28431d63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0739f64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be96974f",
   "metadata": {},
   "source": [
    "# NeuralNetwork - One Hidden Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7939bb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a918a455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((4, 2), (4, 1))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Implementing XOR\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "Y = np.array([[0,1,1,0]]).T\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "724c637a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sig(z):\n",
    "    return 1/(1+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a1fe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivativeSig(z):\n",
    "    return sig(z)*(1-sig(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a32df203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hidden layer weights\n",
    "wh = 2 * np.random.random((2,2)) - 1    \n",
    "bh = 2 * np.random.random((1,2)) - 1   \n",
    "wo = 2 * np.random.random((2,1)) - 1    \n",
    "bo = 2 * np.random.random((1,1)) - 1    \n",
    "lr = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738a7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for iter in range(10000):\n",
    "#     output0 = x\n",
    "#     output = sig(np.dot(output0,weights)+ bias)\n",
    "\n",
    "#     first_term = output - y\n",
    "#     input_for_last_layer = np.dot(output0,weights)+ bias\n",
    "#     second_term = derivativeSig(input_for_last_layer)  # it is same as output*(1-output)\n",
    "#     first_two = first_term*second_term\n",
    "#     first_two.shape\n",
    "\n",
    "#     changes = np.dot(output0.T, first_two)\n",
    "    \n",
    "#     weights = weights - lr * changes\n",
    "\n",
    "#     bias_change = np.sum(first_two)\n",
    "#     bias = bias - lr * bias_change\n",
    "    \n",
    "# output = sig(np.dot(x,weights)+bias)\n",
    "# weights, bias, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0686cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.02259906],\n",
       "        [0.97664597],\n",
       "        [0.97664566],\n",
       "        [0.02846948]]),\n",
       " array([[-5.19446305, -5.56186702],\n",
       "        [-5.19463272, -5.56223181]]),\n",
       " array([[7.76104882, 2.21747942]]),\n",
       " array([[ 8.80177776],\n",
       "        [-9.36274747]]),\n",
       " array([[-4.12161586]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward propogation with one hidden layer\n",
    "for iter in range(10000):\n",
    "    output0 = X\n",
    "    inputHidden = np.dot(output0,wh)+ bh\n",
    "    outputHidden = sig(inputHidden)\n",
    "    inputForOutputLayer = np.dot(outputHidden , wo) + bo\n",
    "    output = sig(inputForOutputLayer)\n",
    "\n",
    "\n",
    "    first_term_output_layer = output-Y\n",
    "    second_term_output_layer = derivativeSig(inputForOutputLayer)\n",
    "    first_two_output_layer = first_term_output_layer*second_term_output_layer\n",
    "\n",
    "    first_term_hidden_layer = np.dot(first_two_output_layer,wo.T)\n",
    "    second_term_hidden_layer = derivativeSig(inputHidden)\n",
    "    first_two_hidden_layer = first_term_hidden_layer*second_term_hidden_layer\n",
    "\n",
    "    # You can compare with the code of w/o output layer to understand\n",
    "    changes_output = np.dot(outputHidden.T, first_two_output_layer)\n",
    "    changes_output_bias = np.sum(first_two_output_layer, axis=0, keepdims=True) #It is exactly same, Just for vector dot product we are using keepdims\n",
    "\n",
    "    changes_hidden = np.dot(output0.T, first_two_hidden_layer)\n",
    "    changes_hidden_bias = np.sum(first_two_hidden_layer, axis=0, keepdims=True)\n",
    "\n",
    "    wo = wo - lr*changes_output\n",
    "    bo = bo - lr*changes_output_bias\n",
    "\n",
    "    wh = wh - lr*changes_hidden\n",
    "    bh = bh - lr*changes_hidden_bias\n",
    "    \n",
    "    \n",
    "output0 = X\n",
    "inputHidden = np.dot(output0,wh)+ bh\n",
    "outputHidden = sig(inputHidden)\n",
    "inputForOutputLayer = np.dot(outputHidden , wo) + bo\n",
    "output = sig(inputForOutputLayer)\n",
    "output, wh, bh, wo, bo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
